{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce76a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading dataset\n",
    "from keras.datasets import fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70883b61",
   "metadata": {},
   "source": [
    "### 1. **clipvalue (Value Clipping)**\n",
    "- **What it does**: Clips each **individual gradient element** to a maximum/minimum value.\n",
    "- **Operation**: Applied element-wise to every single number in the gradient tensor.\n",
    "- **Formula**:  \n",
    "$$\n",
    "g_{\\text{clipped}} = \\max\\!\\bigl(\\min(g,\\; \\text{clip\\_value}),\\; -\\text{clip\\_value}\\bigr)\n",
    "$$\n",
    "  Where `clip_value` is your threshold (e.g., 0.5).\n",
    "- **Effect**: Prevents any single weight from getting an update larger than `Â±clip_value`.\n",
    "- **Use case**: Less common now. Can be too aggressive as it doesn't consider the overall gradient direction.\n",
    "\n",
    "**Example**: If `clip_value=0.5` and a gradient is `[1.2, -0.3, -0.8]`, it becomes `[0.5, -0.3, -0.5]`.\n",
    "\n",
    "### 2. **clipnorm (Norm Clipping)**\n",
    "- **What it does**: Scales the **entire gradient vector** when its norm exceeds a threshold.\n",
    "- **Operation**: Works on the whole vector to preserve the gradient direction.\n",
    "- **Formula**:  \n",
    "$$\n",
    "g_{\\text{clipped}} = \\frac{\\max\\_norm}{\\|g\\|_2} \\cdot g \\quad \\text{if} \\quad \\|g\\|_2 > \\max\\_norm\n",
    "$$\n",
    "- **Effect**: Controls the total step size while maintaining the original update direction (just shorter).\n",
    "- **Use case**: **Most common and recommended**. Preserves the gradient's directional information.\n",
    "\n",
    "**Example**: If gradients are `[3, 4]` (norm = 5) and `max_norm=2`, they become `[1.2, 1.6]` (norm = 2).\n",
    "\n",
    "### Quick Comparison\n",
    "| Feature | **clipvalue** | **clipnorm** |\n",
    "|---------|--------------|--------------|\n",
    "| **What it limits** | Individual gradient values | Overall gradient magnitude |\n",
    "| **Preserves direction** | âŒ No (distorts direction) | âœ… Yes (only scales) |\n",
    "| **Typical threshold** | 0.1 to 1.0 | 0.5 to 5.0 |\n",
    "| **Common usage** | Less common today | Standard practice |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459227f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m784\u001b[0m)              â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              â”‚       \u001b[38;5;34m100,480\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m10\u001b[0m)               â”‚         \u001b[38;5;34m1,290\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Building\n",
    "model = Sequential([\n",
    "    Input(shape=[28, 28], batch_size=32),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7424 - loss: 17.6699 - val_accuracy: 0.7289 - val_loss: 26.9553\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7792 - loss: 20.3557 - val_accuracy: 0.7597 - val_loss: 29.6934\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7738 - loss: 35.6614 - val_accuracy: 0.7103 - val_loss: 75.9841\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7675 - loss: 51.6642 - val_accuracy: 0.6988 - val_loss: 104.1808\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7727 - loss: 63.8006 - val_accuracy: 0.7633 - val_loss: 75.9723\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7737 - loss: 79.9983 - val_accuracy: 0.8001 - val_loss: 70.1591\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7737 - loss: 100.7916 - val_accuracy: 0.8179 - val_loss: 80.4404\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7750 - loss: 117.8692 - val_accuracy: 0.7823 - val_loss: 116.7800\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7790 - loss: 130.0102 - val_accuracy: 0.6749 - val_loss: 232.5338\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7775 - loss: 145.1223 - val_accuracy: 0.7992 - val_loss: 131.3581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x257f5f56f90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(clipvalue=1.0), # All the gradients which either greater than 1 or lesser than -1 are clipped to 1 and -1\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40e011",
   "metadata": {},
   "source": [
    "### ğŸ“ The Exact Formula\n",
    "When the gradient vector's norm exceeds `max_norm`, we scale it down:\n",
    "\n",
    "$$\n",
    "\\mathbf{g} \\leftarrow \\frac{\\text{max\\_norm}}{\\|\\mathbf{g}\\|_2} \\cdot \\mathbf{g} \\quad \\text{if} \\quad \\|\\mathbf{g}\\|_2 > \\text{max\\_norm}\n",
    "$$\n",
    "\n",
    "Here, `||g||â‚‚` in the denominator is calculated as:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{g}\\|_2 = \\sqrt{g_1^2 + g_2^2 + \\dots + g_n^2}\n",
    "$$\n",
    "\n",
    "Where `gâ‚, gâ‚‚, ..., gâ‚™` are the individual gradient values.\n",
    "\n",
    "### ğŸ”¢ Simple Example\n",
    "Letâ€™s say your gradients are `g = [3, 4]` and `max_norm = 2`.\n",
    "\n",
    "1. **Calculate denominator** `||g||â‚‚`:\n",
    "   $$\n",
    "   \\|\\mathbf{g}\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n",
    "   $$\n",
    "\n",
    "2. **Check clipping condition:**\n",
    "   `5 > 2` â†’ Clipping is needed.\n",
    "\n",
    "3. **Apply scaling:**\n",
    "   $$\n",
    "   \\text{Scale factor} = \\frac{\\text{max\\_norm}}{\\|\\mathbf{g}\\|_2} = \\frac{2}{5} = 0.4\n",
    "   $$\n",
    "   $$\n",
    "   \\text{New gradient} = 0.4 \\cdot [3, 4] = [1.2, 1.6]\n",
    "   $$\n",
    "\n",
    "4. **Verify new norm:**\n",
    "   $$\n",
    "   \\|\\text{new\\_g}\\|_2 = \\sqrt{1.2^2 + 1.6^2} = \\sqrt{1.44 + 2.56} = \\sqrt{4} = 2\n",
    "   $$\n",
    "   âœ… Exactly equals `max_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553dd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6930 - loss: 2.8664 - val_accuracy: 0.7219 - val_loss: 0.8919\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7988 - loss: 0.5859 - val_accuracy: 0.8204 - val_loss: 0.5587\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8185 - loss: 0.5398 - val_accuracy: 0.8135 - val_loss: 0.6045\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8284 - loss: 0.5148 - val_accuracy: 0.8015 - val_loss: 0.5831\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8316 - loss: 0.5077 - val_accuracy: 0.8297 - val_loss: 0.5625\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8368 - loss: 0.4963 - val_accuracy: 0.8248 - val_loss: 0.5283\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.4892 - val_accuracy: 0.8204 - val_loss: 0.5397\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8423 - loss: 0.4780 - val_accuracy: 0.8292 - val_loss: 0.5408\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8405 - loss: 0.4863 - val_accuracy: 0.8213 - val_loss: 0.5672\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8446 - loss: 0.4744 - val_accuracy: 0.8324 - val_loss: 0.5507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x257f5fbec10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Clipping using `clipnorm`\n",
    "model = Sequential([\n",
    "    Input(shape=[28, 28], batch_size=32),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(clipnorm=1.0),  # Keeps gradient directions intact while capping total norm to 1.0\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99a11f",
   "metadata": {},
   "source": [
    "### Which to Use?\n",
    "- **Use `clipnorm` (norm clipping) by default** â€“ it's more mathematically sound as it preserves the gradient direction while controlling step size.\n",
    "- Use `clipvalue` only in specific cases where you know you need strict element-wise bounds (rare). Working with extream outliers\n",
    "\n",
    "In practice, most frameworks default to norm clipping because it maintains the optimization trajectory's direction while ensuring stability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64908c83",
   "metadata": {},
   "source": [
    "### âœ… Advantages (Why It's Useful)\n",
    "\n",
    "| Advantage | Explanation |\n",
    "| :--- | :--- |\n",
    "| **1. Prevents Exploding Gradients** | The primary benefit. Stops weights from receiving giant, destabilizing updates that can cause numerical overflow (NaN loss) or make the model diverge. |\n",
    "| **2. Stabilizes Training** | Allows the use of higher learning rates by clipping occasional large updates, often leading to faster convergence. |\n",
    "| **3. Essential for RNNs/Transformers** | Crucial for architectures that process sequences (like LSTMs, GRUs, Transformers) where backpropagation through many time steps (long sequences) can cause gradients to multiply and explode. |\n",
    "| **4. Handles Noisy or Sparse Data (Overfitting)** | Protects against outlier samples or mini-batches that might produce abnormally large gradients. |\n",
    "\n",
    "### âŒ Disadvantages & Risks\n",
    "\n",
    "| Disadvantage | Explanation & Mitigation |\n",
    "| :--- | :--- |\n",
    "| **1. Masks Underlying Problems** | Large gradients can be a symptom of poor initialization, too high a learning rate, or a bug. Clipping might let you train a broken model. **Fix**: Use it alongside good practices, not as a substitute. |\n",
    "| **2. Introduces a Hyperparameter** | The clipping threshold (`max_norm` or `clipvalue`) must be tuned. A bad value can hinder learning (too low) or be ineffective (too high). |\n",
    "| **3. Can Distort the True Gradient Direction** | Scaling the gradient changes its direction. This alters the optimization path, which could theoretically slow convergence. In practice, the stability gain outweighs this. |\n",
    "| **4. Not a Cure-All** | It doesn't help with **vanishing gradients** (where gradients become too small). |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Where and When to Use It (With Examples)\n",
    "\n",
    "You should strongly consider gradient clipping in these scenarios:\n",
    "\n",
    "1.  **Training Recurrent Neural Networks (RNNs)**: This is the **most classic use case**. When you backpropagate through long sequences (many time steps), gradients are multiplied by the same weight matrix repeatedly. This can cause them to grow exponentially (explode).\n",
    "    *   **Example**: Training an LSTM for text generation or machine translation.\n",
    "\n",
    "2.  **Training Deep Transformers**: Similar to RNNs, the deep encoder-decoder structure and attention mechanisms in models like BERT or GPT can suffer from exploding gradients, especially in early training.\n",
    "    *   **Example**: Fine-tuning a large language model (LLM).\n",
    "\n",
    "3.  **Using Very Deep Networks**: Any very deep feedforward network is at risk. Gradient clipping is a standard safety feature.\n",
    "    *   **Example**: Training a 100-layer ResNet variant.\n",
    "\n",
    "4.  **When You See Training Instability**: If your training loss suddenly spikes to `NaN` or shows wild oscillations, exploding gradients are a likely culprit. Gradient clipping is the first line of defense.\n",
    "    *   **Symptom**: Loss goes from 1.5 â†’ `NaN` in one step.\n",
    "\n",
    "5.  **When Using Aggressive Optimizers/Settings**: Optimizers like Adam are less prone to explosion but it can still happen. Clipping is wise when using a high learning rate or large batch sizes.\n",
    "\n",
    "---\n",
    "\n",
    "**Choosing a Threshold (`max_norm`)**:\n",
    "*   **Common Starting Points**: Values between `0.5` and `10.0` are typical. `1.0` or `5.0` are very common defaults.\n",
    "*   **How to Tune**: Monitor your gradient norms during a few training steps. Set the threshold slightly above the **typical range** you observe, so it only clips the pathological outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
