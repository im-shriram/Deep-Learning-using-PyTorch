{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Causes of Vanishing / Exploding Gradients\n",
        "\n",
        "## The Core Problem\n",
        "\n",
        "During backpropagation, gradients are **multiplied** across layers via the chain rule. In deep networks, this repeated multiplication causes gradients to either:\n",
        "- **Shrink exponentially** → Vanishing gradients\n",
        "- **Grow exponentially** → Exploding gradients\n",
        "\n",
        "---\n",
        "\n",
        "## Main Causes\n",
        "\n",
        "### 1. **Activation Function Choice**\n",
        "\n",
        "| Function | Problem | Why |\n",
        "|----------|---------|-----|\n",
        "| Sigmoid | Vanishing | Derivative max is 0.25; squashes values to (0,1) |\n",
        "| Tanh | Vanishing | Derivative max is 1; saturates at extremes |\n",
        "| ReLU | Can cause \"dead neurons\" | Zero gradient for negative inputs |\n",
        "\n",
        "```\n",
        "Sigmoid derivative: σ'(x) = σ(x)(1 - σ(x)) → max = 0.25\n",
        "```\n",
        "\n",
        "### 2. **Poor Weight Initialization**\n",
        "\n",
        "- **Too small weights** → Gradients shrink → Vanishing\n",
        "- **Too large weights** → Gradients grow → Exploding\n",
        "\n",
        "### 3. **Network Depth**\n",
        "\n",
        "```\n",
        "Gradient ∝ (weight × activation_derivative)^n\n",
        "```\n",
        "- If this term < 1 → Vanishes after many layers\n",
        "- If this term > 1 → Explodes after many layers\n",
        "\n",
        "### 4. **Recurrent Neural Networks (RNNs)**\n",
        "\n",
        "- Same weights are multiplied across many time steps\n",
        "- Long sequences make the problem severe\n",
        "\n",
        "---\n",
        "\n",
        "## Solutions\n",
        "\n",
        "| Problem | Solutions |\n",
        "|---------|-----------|\n",
        "| **Vanishing** | ReLU/Leaky ReLU, proper initialization (Xavier/He), Batch Normalization, Skip connections (ResNets), LSTM/GRU for RNNs |\n",
        "| **Exploding** | Gradient clipping, proper initialization, lower learning rate, Batch Normalization |\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
