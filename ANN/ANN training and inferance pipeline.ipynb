{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5a0533",
   "metadata": {},
   "source": [
    "<u>Unresolved Questions</u>\n",
    "\n",
    "- <i>Geometric intuition of perceptron. How decision boundry change after changing the values of weights (Geometrically)? </i>\n",
    "    - Without gradient descent and with gradient descent\n",
    "    \n",
    "- <i>Derive an equation of backpropogation in multiclass classification</i>\n",
    "    - Need to consider two different cases - 1. i == j 2. i != j\n",
    "\n",
    "- <i>When to use `categorical cross-entropy` and when to use `sparse categorical cross-entropy`</i>\n",
    "    - categorical cross-entropy - when the output column is one-hot-encoded\n",
    "    - sparse categorical cross-entropy - when the output column is lable encoded\n",
    "\n",
    "- <i>Why GPU's are preferred to train neural networks over CPU's? What is the difference between TPU and GPU</i>\n",
    "    - \"GPUs are versatile sports cars good for all roads; TPUs are Formula 1 cars that only race on perfect tracks but are unbeatable there.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49331064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms # Transforming images\n",
    "\n",
    "from typing import Tuple, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf8d4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa6a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class with Features\n",
    "class FashionMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class with transforms, validation, and error handling\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 features: torch.Tensor, \n",
    "                 labels: torch.Tensor, \n",
    "                 transform: Optional[Callable] = None, # Callable means it act as a function\n",
    "                 normalize: bool = True,\n",
    "                 mode: str = 'train') -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Feature tensor\n",
    "            labels: Label tensor  \n",
    "            transform: Optional transform to be applied on features\n",
    "            normalize: Whether to normalize pixel values to [0,1]\n",
    "            mode: 'train', 'val', or 'test' - affects data augmentation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input validation\n",
    "        if features.shape[0] != labels.shape[0]:\n",
    "            raise ValueError(f\"Features and labels must have same length. \"\n",
    "                           f\"Got {features.shape} and {labels.shape}\")\n",
    "        \n",
    "        self.features = features\n",
    "        self.labels = labels.long()  # Ensure labels are long type for CrossEntropy (Otherwise error)\n",
    "        self.normalize = normalize\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Set default transforms if none provided\n",
    "        if transform is None:\n",
    "            self.transform = self._get_default_transforms() # Our own transformation function\n",
    "        else:\n",
    "            self.transform = transform # Function variable is passed which is callable\n",
    "            \n",
    "        # Normalize pixel values to [0,1] if requested\n",
    "        if self.normalize:\n",
    "            self.features = self.features.float() / 255.0\n",
    "            \n",
    "        print(f\"Dataset created: {self.mode} mode, {len(self)} samples\")\n",
    "        \n",
    "    def _get_default_transforms(self): # <- private method\n",
    "        \"\"\"Get default transforms based on mode\"\"\"\n",
    "        if self.mode == 'train': # Transformation only applied while training\n",
    "            # Data augmentation for training\n",
    "            return transforms.Compose([\n",
    "                transforms.RandomRotation(degrees=10),\n",
    "                transforms.RandomHorizontalFlip(p=0.1),  # Less common for fashion items\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "            ])\n",
    "        else:\n",
    "            # Only normalization for val/test\n",
    "            return transforms.Compose([\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]) # Normalization is a preprocessing so we need to apply it on test data as well.\n",
    "            ])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get item with error handling\"\"\"\n",
    "        try:\n",
    "            feature = self.features[index]\n",
    "            label = self.labels[index]\n",
    "            \n",
    "            # Apply transforms if any\n",
    "            if self.transform:\n",
    "                feature = self.transform(feature)\n",
    "                \n",
    "            return feature, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading index {index}: {e}\")\n",
    "            # Return a default sample in case of error\n",
    "            return self.features[0], self.labels[0]\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"Get class distribution for analysis\"\"\"\n",
    "        unique, counts = torch.unique(self.labels, return_counts=True) # Also calculates the frequency of each class\n",
    "        return dict(zip(unique.tolist(), counts.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769a2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Splitting Pipeline\n",
    "class DataManager:\n",
    "    \"\"\"\n",
    "    Data management class for loading and splitting\n",
    "    \"\"\"\n",
    "    def __init__(self, train_csv_path: str, test_csv_path: str, random_state: int = 42):\n",
    "        self.train_csv_path = train_csv_path\n",
    "        self.test_csv_path = test_csv_path\n",
    "        self.random_state = random_state\n",
    "        self.class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "        \n",
    "    def load_raw_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load raw CSV data with error handling\"\"\"\n",
    "        try:\n",
    "            train_df = pd.read_csv(self.train_csv_path)\n",
    "            test_df = pd.read_csv(self.test_csv_path)\n",
    "            \n",
    "            print(f\"Loaded training data: {train_df.shape}\")\n",
    "            print(f\"Loaded test data: {test_df.shape}\")\n",
    "            \n",
    "            # Validate data\n",
    "            self._validate_data(train_df, test_df)\n",
    "            \n",
    "            return train_df, test_df\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Error loading data: {e}\")\n",
    "    \n",
    "    def _validate_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        \"\"\"Validate loaded data\"\"\"\n",
    "        # Check for missing values\n",
    "        if train_df.isnull().any().any():\n",
    "            warnings.warn(\"Training data contains missing values\")\n",
    "        if test_df.isnull().any().any():\n",
    "            warnings.warn(\"Test data contains missing values\")\n",
    "            \n",
    "        # Check label range\n",
    "        train_labels = train_df.iloc[:, 0]\n",
    "        test_labels = test_df.iloc[:, 0]\n",
    "        \n",
    "        if train_labels.min() < 0 or train_labels.max() > 9:\n",
    "            raise ValueError(\"Training labels should be between 0-9\") # Specific to this dataset\n",
    "        if test_labels.min() < 0 or test_labels.max() > 9:\n",
    "            raise ValueError(\"Test labels should be between 0-9\")\n",
    "    \n",
    "    def create_stratified_splits(self, \n",
    "                               train_df: pd.DataFrame,\n",
    "                               val_size: float = 0.2,\n",
    "                               stratify: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Create stratified train/validation split\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training dataframe\n",
    "            val_size: Proportion for validation set\n",
    "            stratify: Whether to maintain class distribution\n",
    "        \"\"\"\n",
    "        features = train_df.iloc[:, 1:].values\n",
    "        labels = train_df.iloc[:, 0].values\n",
    "        \n",
    "        if stratify:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                features, labels,\n",
    "                test_size=val_size,\n",
    "                stratify=labels,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "        else:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                features, labels,\n",
    "                test_size=val_size,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "        \n",
    "        # Convert back to DataFrames\n",
    "        train_split = pd.DataFrame(np.column_stack([y_train, X_train]))\n",
    "        val_split = pd.DataFrame(np.column_stack([y_val, X_val]))\n",
    "        \n",
    "        print(f\"Training set: {train_split.shape}\")\n",
    "        print(f\"Validation set: {val_split.shape}\")\n",
    "        \n",
    "        return train_split, val_split\n",
    "    \n",
    "    def prepare_tensors(self, df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Convert DataFrame to tensors with proper shapes\"\"\"\n",
    "        # Separate features and labels\n",
    "        labels = torch.tensor(df.iloc[:, 0].values, dtype=torch.long)\n",
    "        features = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)\n",
    "        \n",
    "        # Reshape features to image format [N, 1, 28, 28]\n",
    "        features = features.reshape(-1, 1, 28, 28)\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    # OPTIONAL\n",
    "    def get_data_statistics(self, features: torch.Tensor, labels: torch.Tensor, name: str):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        print(f\"\\n{name} Dataset Statistics:\")\n",
    "        print(f\"Features shape: {features.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Feature range: [{features.min():.2f}, {features.max():.2f}]\")\n",
    "        \n",
    "        # Class distribution\n",
    "        unique, counts = torch.unique(labels, return_counts=True)\n",
    "        print(\"Class distribution:\")\n",
    "        for class_idx, count in zip(unique, counts):\n",
    "            class_name = self.class_names[class_idx]\n",
    "            print(f\"  {class_idx} ({class_name}): {count} samples ({count/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f02c5d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data: (60000, 785)\n",
      "Loaded test data: (10000, 785)\n",
      "Training set: (51000, 785)\n",
      "Validation set: (9000, 785)\n",
      "\n",
      "Training Dataset Statistics:\n",
      "Features shape: torch.Size([51000, 1, 28, 28])\n",
      "Labels shape: torch.Size([51000])\n",
      "Feature range: [0.00, 255.00]\n",
      "Class distribution:\n",
      "  0 (T-shirt/top): 5100 samples (10.0%)\n",
      "  1 (Trouser): 5100 samples (10.0%)\n",
      "  2 (Pullover): 5100 samples (10.0%)\n",
      "  3 (Dress): 5100 samples (10.0%)\n",
      "  4 (Coat): 5100 samples (10.0%)\n",
      "  5 (Sandal): 5100 samples (10.0%)\n",
      "  6 (Shirt): 5100 samples (10.0%)\n",
      "  7 (Sneaker): 5100 samples (10.0%)\n",
      "  8 (Bag): 5100 samples (10.0%)\n",
      "  9 (Ankle boot): 5100 samples (10.0%)\n",
      "\n",
      "Validation Dataset Statistics:\n",
      "Features shape: torch.Size([9000, 1, 28, 28])\n",
      "Labels shape: torch.Size([9000])\n",
      "Feature range: [0.00, 255.00]\n",
      "Class distribution:\n",
      "  0 (T-shirt/top): 900 samples (10.0%)\n",
      "  1 (Trouser): 900 samples (10.0%)\n",
      "  2 (Pullover): 900 samples (10.0%)\n",
      "  3 (Dress): 900 samples (10.0%)\n",
      "  4 (Coat): 900 samples (10.0%)\n",
      "  5 (Sandal): 900 samples (10.0%)\n",
      "  6 (Shirt): 900 samples (10.0%)\n",
      "  7 (Sneaker): 900 samples (10.0%)\n",
      "  8 (Bag): 900 samples (10.0%)\n",
      "  9 (Ankle boot): 900 samples (10.0%)\n",
      "\n",
      "Test Dataset Statistics:\n",
      "Features shape: torch.Size([10000, 1, 28, 28])\n",
      "Labels shape: torch.Size([10000])\n",
      "Feature range: [0.00, 255.00]\n",
      "Class distribution:\n",
      "  0 (T-shirt/top): 1000 samples (10.0%)\n",
      "  1 (Trouser): 1000 samples (10.0%)\n",
      "  2 (Pullover): 1000 samples (10.0%)\n",
      "  3 (Dress): 1000 samples (10.0%)\n",
      "  4 (Coat): 1000 samples (10.0%)\n",
      "  5 (Sandal): 1000 samples (10.0%)\n",
      "  6 (Shirt): 1000 samples (10.0%)\n",
      "  7 (Sneaker): 1000 samples (10.0%)\n",
      "  8 (Bag): 1000 samples (10.0%)\n",
      "  9 (Ankle boot): 1000 samples (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Data Manager and Load Data\n",
    "data_manager = DataManager(\n",
    "    train_csv_path=\"data/fashion-mnist_train.csv\",\n",
    "    test_csv_path=\"data/fashion-mnist_test.csv\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Load raw data\n",
    "train_df, test_df = data_manager.load_raw_data()\n",
    "\n",
    "# Create stratified train/validation split\n",
    "train_split, val_split = data_manager.create_stratified_splits(\n",
    "    train_df, \n",
    "    val_size=0.15,  # 15% for validation\n",
    "    stratify=True\n",
    ")\n",
    "\n",
    "# Prepare tensors\n",
    "train_features, train_labels = data_manager.prepare_tensors(train_split)\n",
    "val_features, val_labels = data_manager.prepare_tensors(val_split)\n",
    "test_features, test_labels = data_manager.prepare_tensors(test_df)\n",
    "\n",
    "# Print statistics\n",
    "data_manager.get_data_statistics(train_features, train_labels, \"Training\")\n",
    "data_manager.get_data_statistics(val_features, val_labels, \"Validation\") \n",
    "data_manager.get_data_statistics(test_features, test_labels, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1dc263f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: train mode, 51000 samples\n",
      "Dataset created: val mode, 9000 samples\n",
      "Dataset created: test mode, 10000 samples\n",
      "\n",
      "Class distributions:\n",
      "Training: {0: 5100, 1: 5100, 2: 5100, 3: 5100, 4: 5100, 5: 5100, 6: 5100, 7: 5100, 8: 5100, 9: 5100}\n",
      "Validation: {0: 900, 1: 900, 2: 900, 3: 900, 4: 900, 5: 900, 6: 900, 7: 900, 8: 900, 9: 900}\n",
      "Test: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n"
     ]
    }
   ],
   "source": [
    "# Training dataset with augmentation\n",
    "train_dataset = FashionMNISTDataset(\n",
    "    features=train_features,\n",
    "    labels=train_labels,\n",
    "    mode='train',\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Validation dataset without augmentation\n",
    "val_dataset = FashionMNISTDataset(\n",
    "    features=val_features,\n",
    "    labels=val_labels,\n",
    "    mode='val',\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Test dataset without augmentation\n",
    "test_dataset = FashionMNISTDataset(\n",
    "    features=test_features,\n",
    "    labels=test_labels,\n",
    "    mode='test',\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Print class distributions\n",
    "print(\"\\nClass distributions:\")\n",
    "print(\"Training:\", train_dataset.get_class_distribution())\n",
    "print(\"Validation:\", val_dataset.get_class_distribution())\n",
    "print(\"Test:\", test_dataset.get_class_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created:\n",
      "  Training: 796 batches of size 64\n",
      "  Validation: 71 batches of size 128\n",
      "  Test: 79 batches of size 128\n",
      "  Workers: 0, Pin Memory: True\n",
      "\n",
      "Testing DataLoaders:\n",
      "Training Batch 0:\n",
      "  Images shape: torch.Size([64, 1, 28, 28])\n",
      "  Labels shape: torch.Size([64])\n",
      "  Image range: [-1.000, 1.000]\n",
      "  Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Training Batch 1:\n",
      "  Images shape: torch.Size([64, 1, 28, 28])\n",
      "  Labels shape: torch.Size([64])\n",
      "  Image range: [-1.000, 1.000]\n",
      "  Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Training Batch 2:\n",
      "  Images shape: torch.Size([64, 1, 28, 28])\n",
      "  Labels shape: torch.Size([64])\n",
      "  Image range: [-1.000, 1.000]\n",
      "  Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# DataLoader Configuration\n",
    "class DataLoaderManager:\n",
    "    \"\"\"Manager for creating optimized DataLoaders\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_dataloaders(train_dataset: Dataset, \n",
    "                           val_dataset: Dataset, \n",
    "                           test_dataset: Dataset,\n",
    "                           batch_size: int = 64,\n",
    "                           num_workers: int = None,\n",
    "                           pin_memory: bool = None) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create optimized DataLoaders with industrial settings\n",
    "        \n",
    "        Args:\n",
    "            train_dataset, val_dataset, test_dataset: Dataset objects\n",
    "            batch_size: Batch size for training\n",
    "            num_workers: Number of worker processes\n",
    "            pin_memory: Whether to pin memory for GPU transfer\n",
    "        \"\"\"\n",
    "        \n",
    "        # Auto-configure num_workers and pin_memory based on system\n",
    "        if num_workers is None:\n",
    "            num_workers = 0\n",
    "            # num_workers = min(4, os.cpu_count())  # Conservative default\n",
    "            \n",
    "        if pin_memory is None:\n",
    "            pin_memory = torch.cuda.is_available()\n",
    "        \n",
    "        # Training DataLoader - with shuffling\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,  # Always shuffle training data\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True,  # Drop incomplete batches for consistent batch norm\n",
    "            persistent_workers=num_workers > 0  # Keep workers alive\n",
    "        )\n",
    "        \n",
    "        # Validation DataLoader - no shuffling, potentially larger batch\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=batch_size * 2,  # Can use larger batch for inference\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        # Test DataLoader - no shuffling, potentially larger batch\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        print(f\"DataLoaders created:\")\n",
    "        print(f\"  Training: {len(train_loader)} batches of size {batch_size}\")\n",
    "        print(f\"  Validation: {len(val_loader)} batches of size {batch_size * 2}\")\n",
    "        print(f\"  Test: {len(test_loader)} batches of size {batch_size * 2}\")\n",
    "        print(f\"  Workers: {num_workers}, Pin Memory: {pin_memory}\")\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = DataLoaderManager.create_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Test the DataLoaders\n",
    "print(\"\\nTesting DataLoaders:\")\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    print(f\"Training Batch {batch_idx}:\")\n",
    "    print(f\"  Images shape: {images.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "    print(f\"  Unique labels: {torch.unique(labels).tolist()}\")\n",
    "    \n",
    "    if batch_idx == 2:  # Just show first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 569,226 parameters\n",
      "Trainable parameters: 569,226\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture\n",
    "class ImprovedCustomModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved model architecture with better practices\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple = (1, 28, 28), num_classes: int = 10, dropout_rate: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate flattened input size\n",
    "        self.input_size = np.prod(input_shape)\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1, end_dim=-1), # After flattening, input shape becomes [batch_size, 784] for 28x28 images\n",
    "            \n",
    "            # First hidden layer\n",
    "            nn.Linear(in_features=self.input_size, out_features=512),\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            # Second hidden layer\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            # Third hidden layer\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            # Output layer (NO SOFTMAX - handled by CrossEntropyLoss)\n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization for ReLU networks\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create model\n",
    "model = ImprovedCustomModel(\n",
    "    input_shape=(1, 28, 28),\n",
    "    num_classes=10,\n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5 epochs...\n",
      "\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:07<00:00, 11.78it/s, Loss=0.8317, Acc=60.68%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 55.14it/s, Loss=0.4429, Acc=77.91%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3248, Train Acc: 60.68%\n",
      "Val Loss: 0.6067, Val Acc: 77.91%\n",
      "New best validation accuracy: 77.91%\n",
      "\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:01<00:00, 12.86it/s, Loss=0.6742, Acc=70.13%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 59.22it/s, Loss=0.3809, Acc=79.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8086, Train Acc: 70.13%\n",
      "Val Loss: 0.5413, Val Acc: 79.19%\n",
      "New best validation accuracy: 79.19%\n",
      "\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:01<00:00, 12.97it/s, Loss=0.4860, Acc=73.38%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 60.05it/s, Loss=0.3555, Acc=80.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7122, Train Acc: 73.38%\n",
      "Val Loss: 0.5079, Val Acc: 80.61%\n",
      "New best validation accuracy: 80.61%\n",
      "\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:00<00:00, 13.19it/s, Loss=0.5397, Acc=75.03%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 54.42it/s, Loss=0.3347, Acc=81.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6738, Train Acc: 75.03%\n",
      "Val Loss: 0.4866, Val Acc: 81.39%\n",
      "New best validation accuracy: 81.39%\n",
      "\n",
      "Epoch [5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 796/796 [01:02<00:00, 12.83it/s, Loss=0.7258, Acc=76.24%]\n",
      "Validation: 100%|██████████| 71/71 [00:01<00:00, 56.39it/s, Loss=0.3508, Acc=82.27%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6431, Train Acc: 76.24%\n",
      "Val Loss: 0.4687, Val Acc: 82.27%\n",
      "New best validation accuracy: 82.27%\n",
      "\n",
      "Training completed in 319.64 seconds\n",
      "Best validation accuracy: 82.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with Validation\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Training class with validation and monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device, scheduler=None):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            # Move data to device\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (optional but good practice)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc='Validation')\n",
    "            for batch_idx, (images, labels) in enumerate(pbar):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{100.*correct/total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        \"\"\"Complete training loop with validation\"\"\"\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate_epoch()\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "                print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Setup training components\n",
    "learning_rate = 0.001  # Reduced learning rate\n",
    "epochs = 5\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(  # AdamW is often better than Adam\n",
    "    model.parameters(), \n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-4  # L2 regularization\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Create trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "503fa3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 79/79 [00:01<00:00, 75.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Average Loss: 0.4585\n",
      "Accuracy: 82.55%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top      0.733     0.822     0.775      1000\n",
      "     Trouser      0.982     0.959     0.970      1000\n",
      "    Pullover      0.699     0.728     0.713      1000\n",
      "       Dress      0.801     0.905     0.850      1000\n",
      "        Coat      0.736     0.807     0.770      1000\n",
      "      Sandal      0.964     0.834     0.894      1000\n",
      "       Shirt      0.661     0.416     0.511      1000\n",
      "     Sneaker      0.848     0.880     0.864      1000\n",
      "         Bag      0.946     0.947     0.947      1000\n",
      "  Ankle boot      0.871     0.957     0.912      1000\n",
      "\n",
      "    accuracy                          0.826     10000\n",
      "   macro avg      0.824     0.826     0.820     10000\n",
      "weighted avg      0.824     0.826     0.820     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing with Detailed Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Industrial-grade model evaluation class\"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_loader, device, class_names):\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        print(\"Evaluating model on test set...\")\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.test_loader, desc='Testing'):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = 100. * sum(p == l for p, l in zip(all_predictions, all_labels)) / len(all_labels)\n",
    "        avg_loss = total_loss / len(self.test_loader)\n",
    "        \n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Detailed classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        report = classification_report(\n",
    "            all_labels, \n",
    "            all_predictions, \n",
    "            target_names=self.class_names,\n",
    "            digits=3\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'loss': avg_loss,\n",
    "            'predictions': all_predictions,\n",
    "            'labels': all_labels,\n",
    "            'report': report\n",
    "        }\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    class_names=data_manager.class_names\n",
    ")\n",
    "\n",
    "results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fead456",
   "metadata": {},
   "source": [
    "### **Possible Reasons for Sub-100 % Accuracy**\n",
    "\n",
    "<i>Why might the network never reach perfect accuracy?</i>\n",
    "\n",
    "1. **Data-related limitations**\n",
    "    1. **Insufficient training examples** – too few samples prevent the model from learning robust patterns.  \n",
    "    2. **Excessive intrinsic complexity** – the underlying function is highly non-linear or chaotic, exceeding the data’s explanatory power.  \n",
    "    3. **Non-representative sampling** – training set distribution diverges from real-world or test distribution.  \n",
    "    4. **Irreducible label noise or measurement error** – random errors in targets set an unavoidable error floor.\n",
    "\n",
    "2. **Model-related constraints**\n",
    "    1. **Mismatched inductive bias** – architectural assumptions (linearity, locality, etc.) clash with true data structure.  \n",
    "    2. **Under-capacity** – model too simple to express the necessary decision boundaries.  \n",
    "    3. **Sub-optimal hyper-parameters** – learning rate, depth, width, regularization, etc. are not tuned to the problem’s sweet spot.\n",
    "\n",
    "3. **Operational & economic barriers**\n",
    "    1. **Limited compute/GPU budget** – restricted training time or hardware caps achievable convergence.  \n",
    "    2. **Diminishing returns** – each extra 1 % accuracy may demand exponentially more data, parameters, or FLOPs.  \n",
    "    3. **Annotation inconsistencies** – conflicting or subjective labels inject noise that no optimizer can eliminate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
