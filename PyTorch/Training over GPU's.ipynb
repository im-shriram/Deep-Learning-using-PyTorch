{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e729d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac56bbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce MX350'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device Agnostic code\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff5a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomMNIST(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        super().__init__()\n",
    "        self.features = torch.from_numpy(features).to(torch.float32) # NOTE: Datatype of features must be in float format\n",
    "        self.labels = torch.from_numpy(labels).to(torch.long) # NOTE: Datatype of labels must be in long format (required for CrossEntropyLoss)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc2e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset objects\n",
    "train_df = CustomMNIST(features=X_train, labels=y_train)\n",
    "test_df = CustomMNIST(features=X_test, labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d75609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader for train and test datasets\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_df,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_df,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ab5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building \n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "class CustomModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=784, out_features=32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=32, out_features=16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=16, out_features=10)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization for ReLU networks\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5938837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "230800ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model object and shifting to cuda\n",
    "model = CustomModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56760e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = learning_rate, betas=(0.9, 0.999), weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039ee4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Loss: 30.247188794581096\n",
      "Epoch: 2 , Loss: 2.2808089444478354\n",
      "Epoch: 3 , Loss: 2.226473060798645\n",
      "Epoch: 4 , Loss: 2.1806199242273965\n",
      "Epoch: 5 , Loss: 2.1594092527389526\n",
      "Epoch: 6 , Loss: 2.0873604164123534\n",
      "Epoch: 7 , Loss: 2.041416114807129\n",
      "Epoch: 8 , Loss: 1.0762319830497107\n",
      "Epoch: 9 , Loss: 0.4404040053407351\n",
      "Epoch: 10 , Loss: 0.2947535122613112\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        # Shifting to GPU\n",
    "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "\n",
    "        # Forward propogation\n",
    "        y_pred = model(batch_features)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_pred, batch_labels)\n",
    "\n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss/len(train_loader)\n",
    "    print(f'Epoch: {epoch + 1} , Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b34aee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enabling evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f9f1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9268\n"
     ]
    }
   ],
   "source": [
    "# evaluation on test data\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for batch_features, batch_labels in test_loader:\n",
    "    # Shift data over GPU's\n",
    "    batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "\n",
    "    # Calculating predictions\n",
    "    outputs = model(batch_features)\n",
    "\n",
    "    # Predicted classes\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    total += batch_labels.shape[0]\n",
    "    correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "print(correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
