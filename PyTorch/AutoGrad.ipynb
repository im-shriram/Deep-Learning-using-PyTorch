{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "bm_ra2CG21GV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA GeForce MX350\n",
            "(6, 1)\n"
          ]
        }
      ],
      "source": [
        "# Importing torch and setting the device\n",
        "import torch\n",
        "\n",
        "# Setting GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Properties\n",
        "print(torch.cuda.get_device_name())\n",
        "print(torch.cuda.get_device_capability()) # torch.cuda.get_device_properties()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkMcuWP4Q726"
      },
      "source": [
        "*A PyTorch computational graph is a Directed Acyclic Graph (DAG) that represents the sequence of operations performed on tensors during the forward pass of a model. This graph is dynamically built during runtime, meaning it is constructed on the fly as operations are executed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AutoGrad for Scalers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvMNlqM521TC",
        "outputId": "e030c9e9-ea65-4eb5-b9ec-57c899f26a87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3., device='cuda:0', requires_grad=True)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(3.0, requires_grad=True, device=device) # Enables the differentation wrt this variable (x)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_iSjcNw21cK",
        "outputId": "34091ed1-0444-4d03-dceb-bd4f1093d22d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(9., device='cuda:0', grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = x**2\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6vPdCb3RtqM"
      },
      "source": [
        "*`backward` - Computes the gradient of current tensor wrt graph leaves. The graph is differentiated using the chain rule.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "hsmrBTVv4r1A"
      },
      "outputs": [],
      "source": [
        "y.backward(retain_graph = True)\n",
        "# Computational Graph: X -> square function -> y(leaf node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz4fg9YR4s3V",
        "outputId": "1f6ad734-2434-4b48-8a36-dd618365b555"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6., device='cuda:0')"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad\n",
        "# Differentation of leaf node wrt x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Note: When you run `y.backward()` again (only if `retain_graph = True`) then PyTorch saves the gradient in its memory and add it when you run it again.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "tYHUyJhBgO9d"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(4.0, requires_grad=True)\n",
        "y = 2 * x * (torch.cos(x) ** 2) # Multiplication Rule of Derivative\n",
        "z = torch.sin(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_RxpqkS-EpK",
        "outputId": "b977a770-4533-44e2-9142-593add7912ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., requires_grad=True)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAfr7jCr-Fl8",
        "outputId": "f61dc451-42c0-42d5-b768-cddc9b3f5c67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3.4180, grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mspFqkAS-F3h",
        "outputId": "c166a76b-16a9-4352-85b3-b49cae73fb0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-0.2729, grad_fn=<SinBackward0>)"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO1F2tO9TVx7"
      },
      "source": [
        "`Computational Graph`: X    —→    func(2 * x * cos(x)) = y    —→    func(sin(y)) = z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "L_gisRUH-GNJ"
      },
      "outputs": [],
      "source": [
        "z.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6avVSli-nzp",
        "outputId": "46da7cef-18a0-40bb-fcd7-a7e1d3b8ccd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7924)"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad\n",
        "# y.grad - You only able to calculate the gradient wrt leaf node (the last variable) only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvnASV5yUwga"
      },
      "source": [
        "*Gradients of non leaf node(z) will not be calculated*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FktaYAR5XAXm"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Basic AutoGrad from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "IDA7vJD6TERe"
      },
      "outputs": [],
      "source": [
        "# Inputs\n",
        "x = torch.tensor(6.7)  # Input feature\n",
        "y = torch.tensor(0.0)  # True label (binary)\n",
        "\n",
        "w = torch.tensor(1.0)  # Weight\n",
        "b = torch.tensor(0.0)  # Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "CNcSnxKFVw6_"
      },
      "outputs": [],
      "source": [
        "# Binary Cross-Entropy Loss for scalar\n",
        "def binary_cross_entropy_loss(prediction, target):\n",
        "    epsilon = 1e-8  # To prevent log(0)\n",
        "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
        "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Ysa6OOlAVzkI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7012)"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Forward pass\n",
        "z = w * x + b  # Weighted sum (linear part)\n",
        "y_pred = torch.sigmoid(z)  # Predicted probability\n",
        "\n",
        "# Compute binary cross-entropy loss\n",
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "N31_2LUfV2cb"
      },
      "outputs": [],
      "source": [
        "# Derivatives:\n",
        "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
        "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: z with respect to w and b\n",
        "dz_dw = x  # dz/dw = x\n",
        "dz_db = 1  # dz/db = 1 (bias contributes directly to z)\n",
        "\n",
        "# Chain Rule\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSP5rszqV5GG",
        "outputId": "e0447dbc-ad2c-478c-dd99-0bec61462499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
            "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
          ]
        }
      ],
      "source": [
        "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
        "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "3xdGCYz8V-Rh"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvuriIW3a9OY",
        "outputId": "86d312d0-de6a-4929-acc9-b31acb83f2b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Forward Pass\n",
        "z = w*x + b\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUEAvbpcbBlU",
        "outputId": "8e936c70-fc85-43b6-c992-03ae8f043795"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predict Probabilities\n",
        "y_pred = torch.sigmoid(z)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKaUqr18bIBd",
        "outputId": "49f9868a-b65a-44b7-96a1-b697b306c0ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cokZvi7AWXt_"
      },
      "source": [
        "`Computational Graph`: w, b    —→    foreard pass(x, w, b) = z    —→    probabilities(z) = y_pred    —→    loss(y_pred, y_actual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "Q891gtHabNoB"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O27utwfFbiw1",
        "outputId": "21d9ef8d-0fd7-4a38-d2e0-f64d97caf5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ],
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9khZ9vvgXEeo"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AutoGrad for Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuKGIgDThWq1",
        "outputId": "e344ed80-7b2d-4c47-9bff-623b0d3f0674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 2.0000, 3.0000],\n",
              "        [2.5000, 9.8000, 5.7000]], requires_grad=True)"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Gradients with multiple X values\n",
        "x = torch.tensor([[1.0, 2.0, 3.0], [2.5, 9.8, 5.7]], requires_grad = True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFDXqOw7ikIM",
        "outputId": "bb918f6e-c985-47a7-81a6-7836e4680665"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(24.7967, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = (x**2).mean()\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "tWE_JM2xio9Q"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le70Drbkit_2",
        "outputId": "2f7e128f-03b6-413f-f712-107d666203ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.3333, 0.6667, 1.0000],\n",
              "        [0.8333, 3.2667, 1.9000]])"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHsq_WQ1XnWC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gradient Accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcUAtKSFivZ1",
        "outputId": "33ae0b36-b9d4-4f7a-8943-32602204538b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# clearing grad\n",
        "x = torch.tensor(2.0, requires_grad = True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGml8iABkLUf",
        "outputId": "c414b60f-2eca-4615-864e-915d01c32654"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = x ** 2\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "1EkADF0enBdi"
      },
      "outputs": [],
      "source": [
        "y.backward(retain_graph=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60O_yfrInD_T",
        "outputId": "c5df5e1a-a869-436d-8a5b-942f154596d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfBL7ZiEXwup"
      },
      "source": [
        "*The problem with executing `y.backward()` and `x.grad` muptiple times will store the gradient values in the memory → If you run this more than one time then ouptut will be `d1 + d2 + ...` - The values of gradients will get added*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxQMMaFqoB4u",
        "outputId": "e510946a-73ad-40e0-943f-be9bc4b316fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inplace operation\n",
        "x.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfAqJP6YTop"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhv13mEiYWkD"
      },
      "source": [
        "#### Disable gradient tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGyP-K6ooelo",
        "outputId": "723f68be-f97c-4602-8d1d-c7a301ecdbee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x ** 2\n",
        "\n",
        "y.backward(retain_graph=True)\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-3J1W7BsLiK",
        "outputId": "049622cf-e0ab-4bdb-c048-dea79ebf81e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Way 1: requires_grad_ - perminantly disable the gradients\n",
        "x.requires_grad_(False) # requires_grad = False (_ inplace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eqdqlwzsPR_",
        "outputId": "f40eb300-0d6b-4e2f-bcad-53cad5cd96af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ],
      "source": [
        "y = x ** 2 # Calculating y again after updating the state of x\n",
        "\n",
        "try:\n",
        "    y.backward()\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Ct0xomsgw7",
        "outputId": "c124e1db-46db-4aac-8710-bc023c9a385d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x ** 2\n",
        "\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1EMqqbpZT8U",
        "outputId": "2eb1117e-4197-409c-85c6-b07beb5ec347"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Way 2: detach() - The variable gets detached from the DAG\n",
        "z = x.detach() # Copy the value of x but its requires_grad becomes false\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zK-PB97su18",
        "outputId": "f53ca690-8250-44e4-d5b5-0c697e663efe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = z ** 2\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvhtrhXMszVu",
        "outputId": "25a1fb88-58f6-4c5b-9ef2-c44b30211d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    y.backward()\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.8000)\n",
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ],
      "source": [
        "# Consider you are training your neural network\n",
        "x = torch.tensor([1.0, 2.0], requires_grad=True) # Only Tensors of floating point and complex dtype can require gradients\n",
        "x = torch.tensor(2.4, requires_grad=True)\n",
        "y = x ** 2\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "# The neural network is trained\n",
        "\n",
        "# Its prediction time\n",
        "with torch.no_grad():\n",
        "    # In this scope only, all the parameter's requires_grad parameter becomes false\n",
        "    y = x + 10\n",
        "\n",
        "    try:\n",
        "        y.backward()\n",
        "        print(x.grad())\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "# For an instance, need to run a block of code without building a DAG, then you can stop the gradients by using context manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*`torch.no_grad()` is the most recommended way to stop calculating or tracking grad*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
